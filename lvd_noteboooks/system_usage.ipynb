{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Author:** J. Žovák, `482857@mail.muni.cz`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "988ff873aaac245e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a75b337d1c5c48ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learned Vector Database (LVD) Usage Notebook\n",
    "This notebook provides demonstration of the LVD functionalities.\n",
    "First, the creation of collection and uploading of the data into it is shown.\n",
    "After the data upload, the index is built and visualised.\n",
    "Next, all the supported search query types that the LVD supports are presented.\n",
    "Currently, the following query types are supported regular kANN search query, constrained search query and hybrid search query."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eee1f8b55faeaeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from demo_utils import visualize_dataset, plot_bucket_items, visualize_bucket_order"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset\n",
    "In this demo a dataset called `synthetic_clusters_colored.csv` is used to present capabilities of LVD. \n",
    "This dataset contains coordinates of points that form 4 clusters in 2d space. \n",
    "Points have assigned color based on their cluster. \n",
    "The points have additionally a document assigned to them. The document is a one sentence about some topic. Each cluster contains sentences about particular topic."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42290f51a6455b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_file_path = 'data/synthetic_clusters_colored.csv'\n",
    "data = pd.read_csv(csv_file_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "253c2816f88051b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pick Query Point\n",
    "A query point is selected which will be used across different search operations. \n",
    "This will allow us to observe of how the output of the search changes based on the search operation. \n",
    "Feel free to change the query point."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc3c079f6dd38a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_color = \"purple\"\n",
    "query_point = data[data['cluster'] == query_color].iloc[0]\n",
    "print(\"Selected query point: \\n\", query_point)\n",
    "query_point = query_point[['x', 'y']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17f6049d3d9628be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize Data\n",
    "Bellow is the visualization of the dataset with the selected query. From visualization, it can be seen that the dataset contains 4 distinct clusters. Each cluster has a color assigned to it and documents within the cluster are about particular topic. Note the vectors are generated randomly and are **not** created by embedding the documents.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0fabbdd0194aa3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_dataset(data, query_point)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cbcffaceee7deeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Up Database And Collection\n",
    "Now we will connect to the database and create a collection for the synthetic dataset. Then we will upload the data to the database. After the data is uploaded we will build the index."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87f3bc15fe615584"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connect to the database and delete previously created collections."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a676886b33d4a92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Delete previously created collections\n",
    "collections = client.list_collections()\n",
    "if collections:\n",
    "    client.delete_collection(collections[0].name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4fb7f4c17ab0c40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create collection for the synthetic dataset. Specify LMI configuration that will be used for the collection."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fa402d3e1bf0b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection_name = \"synthetic_collection\"\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\n",
    "        \"lmi:epochs\": \"[200]\",\n",
    "        \"lmi:model_types\": \"['MLP']\",\n",
    "        \"lmi:lrs\": \"[0.01]\",\n",
    "        \"lmi:n_categories\": f\"[4]\",\n",
    "        \"lmi:kmeans\": \"{'verbose': False, 'seed': 2023, 'nredo': 10}\",\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c3fba64a0da152"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upload data in batches to the collection."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37d0474889a5350a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use batch upload just to test it out\n",
    "batch_size = 25\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Adding documents\"):\n",
    "    collection.add(\n",
    "        embeddings=data[['x', 'y']].iloc[i: i + batch_size].values.tolist(),\n",
    "        metadatas=[{\"cluster\": cluster} for cluster in data['cluster'].iloc[i: i + batch_size]],\n",
    "        ids=data['id'].iloc[i: i + batch_size].values.tolist(),\n",
    "        documents=[document for document in data['document'].iloc[i: i + batch_size]]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3793c4cfa6eabfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the LMI index over the uploaded data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c08f962574c186c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bucket_assignment = collection.build_index()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b1c384eac099135"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize LMI Buckets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a257deb2623717"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Map the ids in data to buckets using bucket_labels_new_format (assuming this exists outside this function)\n",
    "data['bucket'] = data['id'].map(lambda x: list(bucket_assignment.get(x, [])))\n",
    "data['bucket_str'] = data['bucket'].apply(lambda x: str(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f454d09b56b6204"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize buckets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f0e38d9da32d82e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize how the LMI indexed the data by looking at distribution of items in the buckets (leaves of the tree)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45bef60f82cdeae6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bucket_items(data, False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba1c059d5e12f789"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's look at items in the bucket by their color."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e69c7988535339a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bucket_items(data, True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ef64a67289f1726"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the visualization above it can be seen that each bucket contains items for exactly one cluster."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed7613087acb1da3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regular kANN search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2713caa806441d79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bellow we perform kANN search. We search for 5 objects in the collection that are the most similar to our query. In the query we specify the query vector itself with `query_embeddings`, the desired output format with `include`, number of items we want to retrieve with `n_results` and finally number of buckets to search with `n_buckets`. The `n_buckets` is a search hyperparameter for LMI, the more buckets we decide to search the more precise answer we get, but at the same with more buckets searched the longer the search takes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a22580723c14580"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=list(query_point),\n",
    "    include=[\"metadatas\", 'distances', \"documents\"],\n",
    "    n_results=5, # Specifies k objects to retireve from the collection\n",
    "    n_buckets=1, # Number of buckets LMI is supposed to search through\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bc3a2a68fb7660e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bellow we can see the result of our query, we retrieved 5 most similar objects to the query. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a805f84b1544c7d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Ids: \", results['ids'])\n",
    "print(\"Distances\", results['distances'])\n",
    "print(\"Metadata: \" ,results['metadatas'])\n",
    "print(\"Documents: \",results['documents'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3c7ff5f37c2598e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constrained Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27d80cb50afaf95d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's perform constrained search by stating that the objects in the result set should statisfy the following condition `cluster_color == \"red\"`. This condition is specified in the `where` argument bellow. For constrained search there are three additional arguments that affect the behaviour of the similarity search. `bruteforce_threshold` specifies at what percentage of the dataset a bruteforce search should be used instead of LMI (if unspecified bruteforce is used if after applying where condition less than 20 000 objects remain). Next is `constraint_weight` which specify how much the buckets with objects satisfying the conditions should be prioritised during navigation in the LMI. Lastly there is `search_until_bucket_not_empty` which if set to `True` the LMI will search more than `n_buckets` if no objects satisfying the condition where found in the first `n_buckets`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55021d935aaf8587"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ae6ca813547a90c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "filter_color = \"red\"\n",
    "results = collection.query(\n",
    "    query_embeddings=list(query_point),\n",
    "    include=[\"metadatas\", 'documents', 'distances'],\n",
    "    where={\"cluster\": filter_color},\n",
    "    n_results=5,\n",
    "    n_buckets=1,\n",
    "    bruteforce_threshold=0.0, \n",
    "    constraint_weight=0.0,\n",
    "    search_until_bucket_not_empty=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8925cdb6ae9eef98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Ids: \", results['ids'])\n",
    "print(\"Distances\", results['distances'])\n",
    "print(\"Metadata: \" ,results['metadatas'])\n",
    "print(\"Documents: \",results['documents'])\n",
    "print(\"Constraint Weight Used: \",results['constraint_weight'])\n",
    "print(\"LMI Bucket Order: \", results['bucket_order'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9868b8673ce2553"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `constraint_weight` parameter influences the resulting bucket order (list) which determines in which order the buckets are searched. Since the value of this parameter is set to `0.0` the buckets are ordered based on their similarity to the query. But in the case of constrained search ordering just based on similarity might not be enough since bucket with object satisfying the condition may not be at the begging of the list. Hence, we might miss them (`search_until_bucket_not_empty` is set to `False` since in real applications we can not afford to search many buckets) or the search can take very long time to find them (`search_until_bucket_not_empty` is set to `True`)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff2c55bc6af4d7fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_bucket_order(data, results['bucket_order'][0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d384a0d2b3b5c3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets set `constraint_weight` to value `0.5` and observe how the resulting bucket order will change."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecd0c47e512bde27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "filter_color = \"red\"\n",
    "results = collection.query(\n",
    "    query_embeddings=list(query_point),\n",
    "    include=[\"metadatas\", 'documents', 'distances'],\n",
    "    where={\"cluster\": filter_color},\n",
    "    n_results=5,\n",
    "    n_buckets=1,\n",
    "    bruteforce_threshold=0.0, \n",
    "    constraint_weight=0.5,\n",
    "    search_until_bucket_not_empty=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b9e51f9d442ac1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Ids: \", results['ids'])\n",
    "print(\"Distances\", results['distances'])\n",
    "print(\"Metadata: \" ,results['metadatas'])\n",
    "print(\"Documents: \",results['documents'])\n",
    "print(\"Constraint Weight Used: \",results['constraint_weight'])\n",
    "print(\"LMI Bucket Order: \", results['bucket_order'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f74c9d65bb549bd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see with `constraint_weight` set to `0.5` the bucket that we are interested in gets prioritized to the begging of the bucket order. Picking optimal value for `constraint_weight` can be tricky. Since if it is too high we won't search based on similarity at all leading to bad precision. Setting it too low may cause us to miss the buckets that contain object satisfying the condition. Based on the experiments it is good to set it based on selectivity of the condition. If the % of the data remaining after applying the condition is high the value of `constraint_weight` should be low and vice versa. This is actually how the `constraint_weight` behaves if it is set to `-1`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2922519abad576"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_bucket_order(data, results['bucket_order'][0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b331fbd4b3d072"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hybrid Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9a311e5f7c93b79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hybrid search represents combination of keyword search with vector similarity search through reciprocal rank fusion. This type of search gives user greater control over the search results while not specifying a strict condition. It was shown that this type of search improves recall of the document retrieval in [The Chronicles of RAG](https://arxiv.org/abs/2401.07883) paper.  \n",
    "In LVD the hybrid search combines results from LMI with BM25 algrotihm through reciprocal rank fusion."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88a9fd894364234a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=list(query_point),\n",
    "    include=[\"metadatas\", 'distances', \"documents\"],\n",
    "    n_results=5, # Specifies k objects to retireve from the collection\n",
    "    n_buckets=1, # Number of buckets LMI is supposed to search through\n",
    "    where_document={\"$hybrid\":{ \"$hybrid_terms\": [\"digital\", \"data\", \"programming\"]}}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2fcc77304ae007c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see from the result bellow the results contains different ids (as opposed to regular kANN search) and is ordered based on the ranking obtained from the reciprocal rank fusion. The `id97` is first in the result list since it contains terms specified in the `$hybrid_terms\"` and is also similar to the query vector. Also, as can be seen bellow `'id12'` has distance -1 that is because it was retrieved using BM25 algorithm and the LMI."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "371328c9528bd29c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Ids: \", results['ids'])\n",
    "print(\"Distances\", results['distances'])\n",
    "print(\"Metadata: \" ,results['metadatas'])\n",
    "print(\"Documents: \")\n",
    "for doc in results['documents'][0]:\n",
    "    print(doc)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db9bb98883886a46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Manipulation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52c4b79d0220ab07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "541072089732a62d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection.delete(['id59'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6e505e592c4f528"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bucket_assignment = collection.build_index()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98352bba6625dbbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=list(query_point),\n",
    "    include=[\"metadatas\", 'distances', \"documents\"],\n",
    "    n_results=5, # Specifies k objects to retireve from the collection\n",
    "    n_buckets=1, # Number of buckets LMI is supposed to search through\n",
    ")\n",
    "\n",
    "# id59 is no longer be in the results\n",
    "print(\"Ids: \", results['ids'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deba92cf217fdf7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Persistency"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f338c193fe6e269"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "persistentClient = chromadb.PersistentClient()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf273a18fd779704"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection = persistentClient.create_collection(\n",
    "    name=\"persistent_synthetic_collection\",\n",
    "    metadata={\n",
    "        \"lmi:epochs\": \"[200]\",\n",
    "        \"lmi:model_types\": \"['MLP']\",\n",
    "        \"lmi:lrs\": \"[0.01]\",\n",
    "        \"lmi:n_categories\": f\"[4]\",\n",
    "        \"lmi:kmeans\": \"{'verbose': False, 'seed': 2023, 'nredo': 10}\",\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b529a4507b79ba55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use batch upload just to test it out\n",
    "batch_size = 25\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Adding documents\"):\n",
    "    collection.add(\n",
    "        embeddings=data[['x', 'y']].iloc[i: i + batch_size].values.tolist(),\n",
    "        metadatas=[{\"cluster\": cluster} for cluster in data['cluster'].iloc[i: i + batch_size]],\n",
    "        ids=data['id'].iloc[i: i + batch_size].values.tolist(),\n",
    "        documents=[document for document in data['document'].iloc[i: i + batch_size]]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de5cad1e5bd5089c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bucket_assignment = collection.build_index()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "771c47a996581e00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Restart the Jupyter kernel. Then load the data and the index from the disk."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d13a8be577636d4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anotherClient = chromadb.PersistentClient()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b87bffb1e98079"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anotherCollection = anotherClient.get_collection(\"persistent_synthetic_collection\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b55843d30efb83c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = anotherCollection.query(\n",
    "    query_embeddings=list(query_point),\n",
    "    include=[\"metadatas\", 'distances', \"documents\"],\n",
    "    n_results=5, # Specifies k objects to retireve from the collection\n",
    "    n_buckets=1, # Number of buckets LMI is supposed to search through\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "745afdc0e5ccfffc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Ids: \", results['ids'])\n",
    "print(\"Distances\", results['distances'])\n",
    "print(\"Metadata: \" ,results['metadatas'])\n",
    "print(\"Documents: \",results['documents'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a6e60cbe146359a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
