{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Author:** J. Žovák, `482857@mail.muni.cz`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3760ebaecc62d234"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LVD Usage With RAG Architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5cea99e967b66f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q openai\n",
    "!pip install -q langchain\n",
    "!pip install -q datasets==2.15.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90a178a07cc131a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import chromadb as lvd\n",
    "import pandas as pd "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T12:19:25.315768300Z",
     "start_time": "2024-05-04T12:19:23.947498700Z"
    }
   },
   "id": "b43438f42610b84c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize OpenAI API client"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28b0f7c857e241bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purpose of this demo I will use the OpenAI it does not require setting up local LLM mode. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1fbb9f7a2e8dedb"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "API_KEY = \"your-api-key-here\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T13:08:05.423026600Z",
     "start_time": "2024-05-04T13:08:05.408181400Z"
    }
   },
   "id": "fb25e324ff075ed5"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T13:08:07.637797300Z",
     "start_time": "2024-05-04T13:08:07.379248300Z"
    }
   },
   "id": "31fea64ce5d880f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac959305f029cf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this demo we will use the `ai-arxiv-chunked` dataset from Hugging Face. This dataset coontains already pre-chunked arxiv papers.\n",
    "Chunking is a process of splitting documents into smaller parts and is necessary step in RAG pipeline. \n",
    "Thanks to the `ai-arxiv-chunked` we can skip this step for the purpose of this demo."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "574a48fd5b1414cc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n        num_rows: 41584\n    })\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use chunked version of arxiv dataset by James Calam (https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked)\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    'jamescalam/ai-arxiv-chunked',\n",
    ")\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T12:19:37.194910300Z",
     "start_time": "2024-05-04T12:19:29.020463800Z"
    }
   },
   "id": "e2c109f499f83a60"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "pd_data =  pd.DataFrame(data['train'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T12:19:48.048717600Z",
     "start_time": "2024-05-04T12:19:37.185257Z"
    }
   },
   "id": "b44862919503b192"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use only the data from October 2021 and newer since the OpenAI models have training data up to September 2021."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "150039efa23768cb"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "pd_data['published'] = pd.to_datetime(pd_data['published'], format='%Y%m%d')\n",
    "\n",
    "pd_data_new = pd_data[pd_data['published'] >= pd.to_datetime('2021-10-01')]\n",
    "\n",
    "pd_data_new.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T12:19:48.090565500Z",
     "start_time": "2024-05-04T12:19:48.051316500Z"
    }
   },
   "id": "1492ee7ddf6b6e8b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "documents = pd_data_new['chunk'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T12:19:52.442031700Z",
     "start_time": "2024-05-04T12:19:52.421149900Z"
    }
   },
   "id": "7a3850c63f36d711"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LVD Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8709b8d576abeea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this demo a all-MiniLM-L6-v2 model will be used as the embedding function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "762ebc89775a207d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: C:\\Users\\jakub\\anaconda3\\envs\\win_lvd\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary C:\\Users\\jakub\\anaconda3\\envs\\win_lvd\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "embedding_model = embedding_functions.SentenceTransformerEmbeddingFunction(device='cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-04T12:19:48.090565500Z"
    }
   },
   "id": "d9accd793aba262"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create collection with the embedding function and configuration of the LMI."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00eab290033f7b3"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "chroma_client = lvd.Client()\n",
    "collection = chroma_client.create_collection(\n",
    "  name='news', \n",
    "  embedding_function=embedding_functions.DefaultEmbeddingFunction(),\n",
    "  metadata={\n",
    "    \"lmi:n_categories\": f\"[10]\",\n",
    "  }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-04T12:19:52.010726600Z"
    }
   },
   "id": "66b3580a6ced6611"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upload and embedd the documents."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82a3041a0d1787ce"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            LMI Build Config:\n",
      "            {\n",
      "                clustering_algorithms: [<function cluster at 0x0000024491B3D550>],\n",
      "                epochs: [200],\n",
      "                model_types: ['MLP'],\n",
      "                learning_rate: [0.01],\n",
      "                n_categories: [10],\n",
      "            }\n",
      "             \n"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    ids=[f\"id{i}\" for i in range(len(documents))],\n",
    "    documents=documents\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T12:34:38.744535400Z",
     "start_time": "2024-05-04T12:20:07.767347800Z"
    }
   },
   "id": "98f287f70a6ae014"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build LMI index on the embedded documents chunks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27a3bb71f57d3a1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection.build_index()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1edf53d3c9efaf5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAG Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63b7a84d921f6b48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this demo I use `gpt-3.5-turbo`from [OpenAI](https://platform.openai.com/docs/models/gpt-3-5-turbo) which have training data up to September 2021.\n",
    "Thee `llm_pipeline` represent bare-bones call to the OpenAI API with the user prompt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "399595cd914a4088"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def llm_pipeline(prompt, context = \"\"):\n",
    "    additional_context = f\"Answer user prompt based on the following context: {context}.\" if context else \"\"\n",
    "    system_prompt = f\"You are generic chatbot assitant. {additional_context}\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ],\n",
    "      max_tokens=100,\n",
    "      temperature=0.0,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T13:08:16.421757800Z",
     "start_time": "2024-05-04T13:08:16.403751300Z"
    }
   },
   "id": "b212349a4af3bb23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bellow is the definition of the `rag_pipeline` that represents the simple RAG architecture. It takes the user prompt and uses it to perform a search query in the LVD.\n",
    "The result of the search query represents the context that the LLM model will receive. Thanks to this context, the LLM will be able to generate up to date answer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f205d051d731c57"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def rag_pipeline(prompt, keywords):\n",
    "    results = collection.query(\n",
    "        query_texts=[prompt],\n",
    "        include=[\"documents\"],\n",
    "        n_results=5,\n",
    "        n_buckets=2,\n",
    "        where_document={\"$hybrid\":{ \"$hybrid_terms\": keywords}}\n",
    "    )\n",
    "    context = results['documents'][0][0]\n",
    "    \n",
    "    answer = llm_pipeline(prompt, context)\n",
    "    return answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T13:43:45.854489900Z",
     "start_time": "2024-05-04T13:43:45.846004200Z"
    }
   },
   "id": "320f4a6194f7cd15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in production environment RAG would be additionally integrated within a LLM application framework like [LangChain](https://www.langchain.com/) and [LlamaIndex](https://www.llamaindex.ai/)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f5cad92b0fb9ebc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAG Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1168ce899dc5dce"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-04 15:37:48,127][INFO ][httpx] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer: \n",
      " CONDAQA is not a commonly known term in the field of machine learning. It is possible that you may have mistyped or misheard the term. If you can provide more context or clarify the term you are referring to, I would be happy to help you understand its usage in machine learning.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Can you explain to me what CONDAQA can be used for in machine learning?\"\n",
    "\n",
    "llm_answer = llm_pipeline(user_prompt)\n",
    "print(\"LLM Answer: \\n\", llm_answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T13:37:48.143350800Z",
     "start_time": "2024-05-04T13:37:46.345758Z"
    }
   },
   "id": "c94b9d63e1b8956e"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-04 15:43:50,532][INFO ][httpx] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Answer: \n",
      " CONDAQA, which stands for Contrastive Reading Comprehension Dataset for Reasoning about Negation, can be used in machine learning for training and evaluating models that are designed to understand and reason about negation in text. This dataset provides examples where negation plays a crucial role in answering questions, making it a valuable resource for developing natural language processing models that can accurately interpret and respond to negated statements. By using CONDAQA, researchers and practitioners can improve the performance of machine learning models on\n"
     ]
    }
   ],
   "source": [
    "rag_answer = rag_pipeline(user_prompt, [\"CONDAQA\", \"contrastive\"])\n",
    "print(\"RAG Answer: \\n\", rag_answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T13:43:50.542942600Z",
     "start_time": "2024-05-04T13:43:47.694889100Z"
    }
   },
   "id": "4037da18fc68f588"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
